# Cluster Configuration Optimization Solution
## Data-Driven Recommendations Using System Tables and Delta Logs

---

## Table of Contents
1. [Solution Overview](#solution-overview)
2. [Architecture & Data Flow](#architecture--data-flow)
3. [Data Sources](#data-sources)
4. [Metrics Collection](#metrics-collection)
5. [Recommendation Engine](#recommendation-engine)
6. [Implementation Examples](#implementation-examples)

---

## Solution Overview

### Problem Statement
- **Challenge**: Standardized cluster configurations (e.g., Standard_E8s_v3, 16 max workers) applied to all jobs regardless of actual needs
- **Impact**: Significant over-provisioning leading to:
  - Low resource utilization (5-30% efficiency)
  - Unnecessary costs (50-75% potential savings)
  - Poor resource matching for different workload types

### Solution Approach
Our solution analyzes **actual job execution patterns** from:
- **System Tables**: Resource utilization metrics from Databricks system tables
- **Delta Logs**: Data volume and table operations from Delta transaction logs
- **Recommendation Engine**: SQL-based function that recommends optimal cluster configurations

### Key Outcomes
- **Data-Driven Decisions**: Recommendations based on real usage patterns
- **Workload-Aware**: Different recommendations for ETL vs. complex aggregations
- **Cost Optimization**: 30-75% potential cost reduction for underutilized jobs
- **Performance Maintained**: Recommendations ensure adequate capacity for peak demand

---

## Architecture & Data Flow

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Databricks System Tables                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  system.lakeflow.job_run_timeline                               â”‚
â”‚  system.lakeflow.job_task_run_timeline                          â”‚
â”‚  system.compute.node_timeline                                   â”‚
â”‚  system.compute.clusters                                        â”‚
â”‚  system.compute.node_types                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Resource Utilization Analysis Engine                â”‚
â”‚  â€¢ Provisioned Resources (configured max)                       â”‚
â”‚  â€¢ Consumed Resources (actually running/paid)                   â”‚
â”‚  â€¢ Utilized Resources (actual usage)                            â”‚
â”‚  â€¢ Efficiency Metrics (provisioning & utilization)              â”‚
â”‚  â€¢ Peak Metrics (P95/P99, spikes)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Delta Transaction Logs                        â”‚
â”‚  â€¢ rows_added (data volume)                                     â”‚
â”‚  â€¢ num_of_tables (workload complexity)                          â”‚
â”‚  â€¢ operationMetrics                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Cluster Configuration Recommendation                â”‚
â”‚  â€¢ Node Family Selection (D/E/F)                                â”‚
â”‚  â€¢ vCPU Count Selection (2/4/8/16)                              â”‚
â”‚  â€¢ Min/Max Workers Calculation                                  â”‚
â”‚  â€¢ Confidence Score & Rationale                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   JSON Recommendations                          â”‚
â”‚  â€¢ Recommended Configuration                                    â”‚
â”‚  â€¢ Expected Improvements                                        â”‚
â”‚  â€¢ Workload Characteristics                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Sources

### 1. Databricks System Tables

#### Job Execution Tables
```
system.lakeflow.job_run_timeline
â”œâ”€â”€ workspace_id
â”œâ”€â”€ job_id
â”œâ”€â”€ job_run_id
â”œâ”€â”€ compute_ids[0] â†’ cluster_id (extracted from array)
â”œâ”€â”€ period_start_time
â”œâ”€â”€ period_end_time
â””â”€â”€ result_state

system.lakeflow.job_task_run_timeline
â”œâ”€â”€ job_id
â”œâ”€â”€ job_run_id
â”œâ”€â”€ task_key
â”œâ”€â”€ period_start_time
â””â”€â”€ period_end_time
```

#### Compute & Resource Tables
```
system.compute.node_timeline
â”œâ”€â”€ cluster_id
â”œâ”€â”€ instance_id
â”œâ”€â”€ start_time / end_time
â”œâ”€â”€ cpu_user_percent
â”œâ”€â”€ cpu_system_percent
â”œâ”€â”€ cpu_wait_percent
â”œâ”€â”€ mem_used_percent
â”œâ”€â”€ network_sent_bytes
â”œâ”€â”€ network_received_bytes
â””â”€â”€ node_type

system.compute.clusters
â”œâ”€â”€ cluster_id
â”œâ”€â”€ workspace_id
â”œâ”€â”€ worker_count / max_autoscale_workers
â”œâ”€â”€ worker_node_type
â”œâ”€â”€ driver_node_type
â””â”€â”€ change_time

system.compute.node_types
â”œâ”€â”€ node_type
â”œâ”€â”€ core_count
â””â”€â”€ memory_mb
```

### 2. Delta Transaction Logs

```
unity_catalog.schema.delta_table_history
â”œâ”€â”€ catalog_name
â”œâ”€â”€ schema_name
â”œâ”€â”€ table_name
â”œâ”€â”€ version
â”œâ”€â”€ timestamp
â”œâ”€â”€ operation (WRITE, MERGE, DELETE, etc.)
â”œâ”€â”€ job (struct)
â”‚   â”œâ”€â”€ job_id
â”‚   â””â”€â”€ job_run_id
â”œâ”€â”€ clusterId
â”œâ”€â”€ operationMetrics (map)
â”‚   â”œâ”€â”€ numOutputRows â†’ rows_added
â”‚   â”œâ”€â”€ numAddedFiles
â”‚   â””â”€â”€ numRemovedFiles
â””â”€â”€ readVersion
```

---

## Data Extraction Flow

### Detailed Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Job Run Identification                                       â”‚
â”‚ system.lakeflow.job_run_timeline                                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ SELECT job_id, job_run_id, compute_ids[0] as cluster_id         â”‚ â”‚
â”‚ â”‚ WHERE period_start_time >= start_date                            â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Cluster Configuration Lookup                                â”‚
â”‚ system.compute.clusters JOIN system.compute.node_types             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ â€¢ Get cluster config at time of job run                         â”‚ â”‚
â”‚ â”‚ â€¢ Extract worker_count, node_types                              â”‚ â”‚
â”‚ â”‚ â€¢ Calculate provisioned CPUs/memory                             â”‚ â”‚
â”‚ â”‚   = (driver_cores + worker_cores * worker_count)                â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Actual Resource Consumption                                 â”‚
â”‚ system.compute.node_timeline                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ â€¢ Count distinct instances per time period                      â”‚ â”‚
â”‚ â”‚   â†’ avg_nodes_consumed                                          â”‚ â”‚
â”‚ â”‚ â€¢ Calculate CPU utilization                                     â”‚ â”‚
â”‚ â”‚   = (cpu_user + cpu_system + cpu_wait) / 100                    â”‚ â”‚
â”‚ â”‚ â€¢ Calculate Memory utilization                                  â”‚ â”‚
â”‚ â”‚   = mem_used_percent / 100                                      â”‚ â”‚
â”‚ â”‚ â€¢ Capture PEAK values (MAX aggregations)                        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Resource Utilization Calculation                            â”‚
â”‚ Combined: Provisioned + Consumed + Utilized                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Provisioned Resources (Configured Max)                           â”‚ â”‚
â”‚ â”‚   â”œâ”€â”€ total_cpus_provisioned                                    â”‚ â”‚
â”‚ â”‚   â”œâ”€â”€ total_memory_gb_provisioned                               â”‚ â”‚
â”‚ â”‚   â””â”€â”€ max_nodes_provisioned                                     â”‚ â”‚
â”‚ â”‚                                                                 â”‚ â”‚
â”‚ â”‚ Consumed Resources (Actually Running/Paid)                       â”‚ â”‚
â”‚ â”‚   â”œâ”€â”€ avg_cpus_consumed = avg_nodes * cores_per_node            â”‚ â”‚
â”‚ â”‚   â”œâ”€â”€ avg_memory_gb_consumed = avg_nodes * memory_per_node      â”‚ â”‚
â”‚ â”‚   â””â”€â”€ avg_nodes_consumed                                        â”‚ â”‚
â”‚ â”‚                                                                 â”‚ â”‚
â”‚ â”‚ Utilized Resources (Actual Usage)                                â”‚ â”‚
â”‚ â”‚   â”œâ”€â”€ avg_cpus_utilized = consumed * cpu_utilization_pct        â”‚ â”‚
â”‚ â”‚   â””â”€â”€ avg_memory_gb_utilized = consumed * memory_utilization_pctâ”‚ â”‚
â”‚ â”‚                                                                 â”‚ â”‚
â”‚ â”‚ Efficiency Metrics                                                â”‚ â”‚
â”‚ â”‚   â”œâ”€â”€ provisioning_efficiency = (consumed / provisioned) * 100  â”‚ â”‚
â”‚ â”‚   â”œâ”€â”€ cpu_utilization_efficiency = (utilized / consumed) * 100  â”‚ â”‚
â”‚ â”‚   â””â”€â”€ memory_utilization_efficiency = (utilized / consumed) * 100â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: Statistical Aggregation by Job                              â”‚
â”‚ Aggregated across multiple job runs                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ â€¢ AVG(avg_nodes_consumed) â†’ avg_nodes_consumed                  â”‚ â”‚
â”‚ â”‚ â€¢ PERCENTILE_APPROX(avg_nodes_consumed, 0.95) â†’ p95_nodes      â”‚ â”‚
â”‚ â”‚ â€¢ PERCENTILE_APPROX(avg_nodes_consumed, 0.99) â†’ p99_nodes      â”‚ â”‚
â”‚ â”‚ â€¢ MAX(peak_cpu_utilization) â†’ peak_cpu_utilization_pct          â”‚ â”‚
â”‚ â”‚ â€¢ MAX(peak_memory_utilization) â†’ peak_memory_utilization_pct    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: Delta Log Integration                                       â”‚
â”‚ unity_catalog.schema.delta_table_history                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ JOIN on: job.job_id, job.job_run_id, clusterId                  â”‚ â”‚
â”‚ â”‚                                                                 â”‚ â”‚
â”‚ â”‚ â€¢ SUM(operationMetrics.numOutputRows) â†’ rows_added              â”‚ â”‚
â”‚ â”‚ â€¢ COUNT(DISTINCT table_name) â†’ num_of_tables                    â”‚ â”‚
â”‚ â”‚                                                                 â”‚ â”‚
â”‚ â”‚ Purpose:                                                         â”‚ â”‚
â”‚ â”‚ â€¢ Identify data volume (high rows = memory needs)                â”‚ â”‚
â”‚ â”‚ â€¢ Identify complexity (many tables = complex joins)              â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 7: Recommendation Generation                                   â”‚
â”‚ recommend_cluster_config() SQL Function                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Input: All metrics from Steps 1-6                               â”‚ â”‚
â”‚ â”‚                                                                 â”‚ â”‚
â”‚ â”‚ Output: JSON with                                               â”‚ â”‚
â”‚ â”‚   â”œâ”€â”€ recommended_node_type (e.g., Standard_E4s_v3)            â”‚ â”‚
â”‚ â”‚   â”œâ”€â”€ recommended_min_workers                                   â”‚ â”‚
â”‚ â”‚   â”œâ”€â”€ recommended_max_workers                                   â”‚ â”‚
â”‚ â”‚   â”œâ”€â”€ rationale                                                 â”‚ â”‚
â”‚ â”‚   â””â”€â”€ confidence_score                                          â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## System Tables Relationship Diagram

### Entity Relationship Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              system.lakeflow.job_run_timeline                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ workspace_id â”‚  â”‚   job_id     â”‚  â”‚  job_run_id  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ compute_ids[0] â†’ cluster_id (extracted from array)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                  â”‚
         â”‚                                  â”‚
         â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ system.compute.clusters â”‚    â”‚ system.compute.node_timeline     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ cluster_id   â”‚â—„â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”‚ cluster_id   â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚worker_node_typeâ”‚     â”‚    â”‚  â”‚ instance_id              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚    â”‚  â”‚ cpu_user_percent         â”‚   â”‚
â”‚         â”‚               â”‚    â”‚  â”‚ cpu_system_percent       â”‚   â”‚
â”‚         â”‚               â”‚    â”‚  â”‚ cpu_wait_percent         â”‚   â”‚
â”‚         â–¼               â”‚    â”‚  â”‚ mem_used_percent         â”‚   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ â”‚system.compute.   â”‚   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â”‚node_types        â”‚   â”‚
â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚ â”‚  â”‚ node_type  â”‚â—„â”€â”˜   â”‚
â”‚ â”‚  â”‚ core_count â”‚      â”‚
â”‚ â”‚  â”‚ memory_mb  â”‚      â”‚
â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         unity_catalog.schema.delta_table_history                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ job.job_id   â”‚  â”‚  operationMetrics.numOutputRows  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚clusterId     â”‚  â”‚  COUNT(DISTINCT table_name)      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  JOIN Logic           â”‚
        â”‚  â€¢ job.job_id         â”‚
        â”‚  â€¢ job.job_run_id     â”‚
        â”‚  â€¢ clusterId          â”‚
        â”‚  â€¢ timestamp overlap  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Join Strategy

```
Job Run Timeline (job_id, job_run_id, cluster_id)
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                             â”‚
         â–¼                             â–¼
    Clusters Table              Node Timeline
    (cluster_id)                (cluster_id)
         â”‚                             â”‚
         â”‚                             â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚          â”‚
         â–¼          â–¼
    Node Types    Aggregations
    (node_type)   â€¢ AVG nodes consumed
                  â€¢ AVG CPU/memory %
                  â€¢ MAX peak values
                  â€¢ PERCENTILE_APPROX
         â”‚
         â”‚
         â–¼
    Delta Logs (JOIN on job_id, clusterId)
         â”‚
         â”‚
         â–¼
    Final Metrics
    â€¢ Provisioned
    â€¢ Consumed
    â€¢ Utilized
    â€¢ rows_added
    â€¢ num_of_tables
```

---

## Metrics Collection

### Three-Tier Resource Analysis

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIER 1: PROVISIONED RESOURCES (Configured Maximum)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Example: Standard_E8s_v3, Max Workers: 16                   â”‚
â”‚ â€¢ total_cpus_provisioned = 136 (1 driver + 16 workers * 8)  â”‚
â”‚ â€¢ total_memory_gb_provisioned = 1088 GB                     â”‚
â”‚ â€¢ max_nodes_provisioned = 17                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ Actual usage rarely reaches this
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIER 2: CONSUMED RESOURCES (Actually Running/Paid)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Example: Autoscaling to 4 nodes average                     â”‚
â”‚ â€¢ avg_nodes_consumed = 4.0                                  â”‚
â”‚ â€¢ avg_cpus_consumed = 32 CPUs (4 nodes * 8 cores)           â”‚
â”‚ â€¢ avg_memory_gb_consumed = 256 GB                           â”‚
â”‚                                                              â”‚
â”‚ Provisioning Efficiency = (32 / 136) * 100 = 23.5%          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ Actual workload utilization
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIER 3: UTILIZED RESOURCES (Actual Usage from Consumed)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Example: 5% CPU, 9% Memory utilization                      â”‚
â”‚ â€¢ avg_cpus_utilized = 1.6 CPUs (32 * 0.05)                  â”‚
â”‚ â€¢ avg_memory_gb_utilized = 23.04 GB (256 * 0.09)            â”‚
â”‚                                                              â”‚
â”‚ Utilization Efficiency = (1.6 / 32) * 100 = 5%              â”‚
â”‚ Overall Efficiency = (1.6 / 136) * 100 = 1.2%               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Metrics Calculated

#### Utilization Metrics
- **avg_cpu_utilization_pct**: Average CPU utilization across job run
- **avg_memory_utilization_pct**: Average memory utilization across job run
- **peak_cpu_utilization_pct**: Maximum CPU utilization (spike detection)
- **peak_memory_utilization_pct**: Maximum memory utilization (spike detection)

#### Consumption Metrics
- **avg_nodes_consumed**: Average number of nodes running during job execution
- **p95_nodes_consumed**: 95th percentile of nodes consumed (handles 95% of runs)
- **p99_nodes_consumed**: 99th percentile of nodes consumed (handles rare spikes)
- **max_nodes_consumed**: Maximum nodes consumed across all runs

#### Efficiency Metrics
- **provisioning_efficiency_pct**: (consumed / provisioned) * 100
  - Low = over-provisioned, high = right-sized
- **cpu_utilization_efficiency_pct**: (utilized / consumed) * 100
  - Low = idle resources, high = well-utilized
- **memory_utilization_efficiency_pct**: (utilized / consumed) * 100
  - Low = wasted memory, high = efficient usage

#### Workload Characteristics (from Delta Logs)
- **rows_added**: Total rows processed (data volume indicator)
- **num_of_tables**: Number of distinct tables involved (complexity indicator)

---

## Recommendation Engine

### Decision Logic Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: Resource Utilization Metrics                          â”‚
â”‚ â€¢ CPU/Memory utilization percentages                         â”‚
â”‚ â€¢ Nodes consumed (avg, P95, P99)                             â”‚
â”‚ â€¢ Peak utilization spikes                                    â”‚
â”‚ â€¢ rows_added, num_of_tables                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ STEP 1: Workload Classification   â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ IF rows_added > 10M               â”‚
        â”‚    AND num_of_tables â‰¤ 3          â”‚
        â”‚    AND CPU < 20%                  â”‚
        â”‚ â†’ SIMPLE_ETL                      â”‚
        â”‚                                    â”‚
        â”‚ IF num_of_tables > 5              â”‚
        â”‚    AND CPU > 50%                  â”‚
        â”‚ â†’ COMPLEX_AGGREGATION             â”‚
        â”‚                                    â”‚
        â”‚ ELSE â†’ MIXED                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ STEP 2: Node Family Selection     â”‚
        â”‚ (D, E, or F)                      â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ IF memory/CPU ratio > 1.5         â”‚
        â”‚    OR SIMPLE_ETL                  â”‚
        â”‚ â†’ E Family (Memory Optimized)     â”‚
        â”‚                                    â”‚
        â”‚ IF CPU/memory ratio > 1.5         â”‚
        â”‚    OR COMPLEX_AGGREGATION         â”‚
        â”‚ â†’ F Family (Compute Optimized)    â”‚
        â”‚                                    â”‚
        â”‚ ELSE                              â”‚
        â”‚ â†’ D Family (General Purpose)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ STEP 3: vCPU Count Selection      â”‚
        â”‚ (2, 4, 8, or 16)                  â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ Calculate:                        â”‚
        â”‚ CPUs_per_node =                   â”‚
        â”‚   avg_cpus_utilized /             â”‚
        â”‚   avg_nodes_consumed              â”‚
        â”‚                                    â”‚
        â”‚ Map to available sizes:           â”‚
        â”‚ IF â‰¤ 2 â†’ 2 vCPUs                  â”‚
        â”‚ IF â‰¤ 4 â†’ 4 vCPUs                  â”‚
        â”‚ IF â‰¤ 8 â†’ 8 vCPUs                  â”‚
        â”‚ ELSE â†’ 16 vCPUs                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ STEP 4: Worker Count Calculation  â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ Min Workers =                     â”‚
        â”‚   CEIL(avg_nodes * 0.9)           â”‚
        â”‚                                    â”‚
        â”‚ Max Workers =                     â”‚
        â”‚   MAX(                            â”‚
        â”‚     min_workers,                  â”‚
        â”‚     CEIL(p95_nodes * 1.2)         â”‚
        â”‚   )                               â”‚
        â”‚                                    â”‚
        â”‚ Cap at reasonable limit (32)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ OUTPUT: JSON Recommendation       â”‚
        â”‚ â€¢ recommended_node_type           â”‚
        â”‚ â€¢ recommended_min_workers         â”‚
        â”‚ â€¢ recommended_max_workers         â”‚
        â”‚ â€¢ rationale                       â”‚
        â”‚ â€¢ confidence_score                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example Recommendations

#### Example 1: Low Utilization Job (Simple ETL)
```
Input Metrics:
â”œâ”€â”€ avg_cpu_utilization_pct: 4.92%
â”œâ”€â”€ avg_memory_utilization_pct: 9.09%
â”œâ”€â”€ avg_nodes_consumed: 3.5
â”œâ”€â”€ p95_nodes_consumed: 4.0
â”œâ”€â”€ rows_added: 45,700,000
â””â”€â”€ num_of_tables: 2

Recommendation:
â”œâ”€â”€ Family: E (Memory Optimized)
â”œâ”€â”€ vCPUs: 4
â”œâ”€â”€ Node Type: Standard_E4s_v3
â”œâ”€â”€ Min Workers: 2
â”œâ”€â”€ Max Workers: 5
â””â”€â”€ Rationale: "E family for memory-optimized workloads. 
                Low CPU (5%) but needs memory for JSON parsing. 
                Small nodes allow better resource matching."

Expected Cost Reduction: 60-70%
```

#### Example 2: High Utilization Job (Complex Aggregation)
```
Input Metrics:
â”œâ”€â”€ avg_cpu_utilization_pct: 67.13%
â”œâ”€â”€ avg_memory_utilization_pct: 51.49%
â”œâ”€â”€ avg_nodes_consumed: 12.5
â”œâ”€â”€ p95_nodes_consumed: 14.0
â”œâ”€â”€ rows_added: 5,000,000
â””â”€â”€ num_of_tables: 8

Recommendation:
â”œâ”€â”€ Family: F (Compute Optimized)
â”œâ”€â”€ vCPUs: 8
â”œâ”€â”€ Node Type: Standard_F8s_v3
â”œâ”€â”€ Min Workers: 10
â”œâ”€â”€ Max Workers: 14
â””â”€â”€ Rationale: "F family for compute-optimized workloads. 
                High CPU utilization (67%) indicates CPU-bound workload. 
                Current sizing appropriate, optimize node type."

Expected Cost Reduction: 0-10% (already efficient)
```

---

## Implementation Examples

### Example 1: Get Resource Utilization Metrics

```python
from datetime import datetime
from src.data_collection.workflow_metrics_collector_pyspark import WorkflowMetricsCollectorPySpark

# Initialize collector
collector = WorkflowMetricsCollectorPySpark(spark)

# Get resource utilization for jobs
utilization = collector.get_resource_utilization(
    start_date=datetime(2024, 1, 1),
    end_date=datetime(2024, 1, 31),
    job_ids=['job_123', 'job_456'],
    group_by_job=True
)

# Results include:
# - Provisioned resources (configured max)
# - Consumed resources (actually running)
# - Utilized resources (actual usage)
# - Efficiency metrics
# - P95/P99 percentiles
# - Peak utilization spikes
```

### Example 2: Get Cluster Configuration Recommendations

```python
# Get recommendations
recommendations = collector.get_cluster_config_recommendations(
    start_date=datetime(2024, 1, 1),
    end_date=datetime(2024, 1, 31),
    current_node_type="Standard_E8s_v3",
    safety_margin=0.2  # 20% safety margin
)

# Each recommendation includes:
# - recommended_node_type (e.g., "Standard_E4s_v3")
# - recommended_min_workers
# - recommended_max_workers
# - rationale
# - confidence_score
# - expected_cost_reduction_pct
```

### Example 3: Direct SQL Function Call

```sql
-- Call recommendation function directly
SELECT 
    job_id,
    recommend_cluster_config(
        avg_cpu_utilization_pct,
        avg_memory_utilization_pct,
        avg_nodes_consumed,
        p95_nodes_consumed,
        p99_nodes_consumed,
        peak_cpu_utilization_pct,
        peak_memory_utilization_pct,
        avg_cpus_utilized,
        avg_memory_gb_utilized,
        'Standard_E8s_v3',
        max_nodes_provisioned,
        rows_added,
        num_of_tables,
        20.0
    ) AS recommendation_json
FROM job_resource_metrics
WHERE job_id = 'your_job_id';
```

### Example 4: Parse JSON Recommendation

```sql
-- Parse JSON to structured columns
SELECT 
    job_id,
    recommendation_json.recommended_node_type,
    recommendation_json.recommended_min_workers,
    recommendation_json.recommended_max_workers,
    recommendation_json.rationale,
    recommendation_json.confidence_score,
    recommendation_json.expected_improvements.estimated_cost_reduction_pct
FROM (
    SELECT 
        job_id,
        FROM_JSON(
            recommend_cluster_config(...),
            'STRUCT<
                recommended_node_type: STRING,
                recommended_min_workers: INT,
                recommended_max_workers: INT,
                rationale: STRING,
                confidence_score: DOUBLE,
                expected_improvements: STRUCT<estimated_cost_reduction_pct: DOUBLE>
            >'
        ) AS recommendation_json
    FROM job_resource_metrics
);
```

---

## Key Benefits

### 1. Data-Driven Decisions
- **Before**: One-size-fits-all cluster configuration
- **After**: Customized configurations based on actual usage patterns

### 2. Cost Optimization
- **Potential Savings**: 30-75% for underutilized jobs
- **ROI**: Immediate cost reduction without performance impact
- **Focus Areas**: Jobs with <30% utilization efficiency

### 3. Performance Assurance
- **P95/P99 Analysis**: Handles peak demand
- **Safety Margins**: Built-in buffer for unexpected spikes
- **Workload-Aware**: Different recommendations for different job types

### 4. Scalability
- **SQL-Based**: Leverages Databricks SQL engine optimizations
- **Efficient**: Single query execution for all jobs
- **Maintainable**: Clear separation of data collection and recommendation logic

---

## Technical Stack

### Technologies Used
- **Databricks System Tables**: Native observability data
- **Delta Transaction Logs**: Data volume and operation metrics
- **PySpark**: Data processing and DataFrame operations
- **Databricks SQL**: SQL functions and queries
- **JSON**: Recommendation output format

### Key Components
1. **WorkflowMetricsCollectorPySpark**: Main data collection class
2. **recommend_cluster_config()**: SQL function for recommendations
3. **ResourceUtilization**: Data model for metrics
4. **ClusterConfigurationRecommendation**: Data model for recommendations

---

## Next Steps

### Implementation Roadmap
1. âœ… Data collection from system tables
2. âœ… Resource utilization analysis
3. âœ… Delta log integration
4. âœ… Recommendation engine
5. ğŸ”„ Production deployment
6. ğŸ”„ Monitoring and validation
7. ğŸ”„ Continuous optimization

### Success Metrics
- **Cost Reduction**: Track actual savings vs. recommendations
- **Utilization Improvement**: Monitor efficiency metrics over time
- **Performance Impact**: Ensure no performance degradation
- **Adoption Rate**: Track how many jobs implement recommendations

---

## Conclusion

This solution provides a **comprehensive, data-driven approach** to cluster configuration optimization by:

1. **Collecting** real usage data from system tables and Delta logs
2. **Analyzing** three-tier resource utilization (provisioned, consumed, utilized)
3. **Recommending** optimal configurations based on workload patterns
4. **Delivering** actionable recommendations with clear rationale

The result: **Significant cost savings** (30-75%) while maintaining performance through intelligent, workload-aware cluster sizing.

---

## Questions & Discussion

For questions or further details, please refer to:
- `CLUSTER_CONFIGURATION_RECOMMENDATION_DESIGN.md` - Detailed design specifications
- `PBI_BREAKDOWN.md` - Implementation breakdown for developers
- `RESOURCE_UTILIZATION_ANALYSIS.md` - Metrics explanation

